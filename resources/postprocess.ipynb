{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaKIdTFFDMZh",
        "outputId": "1dc13281-5ac4-4606-b4b9-f86f66ea64ca"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Download data from remote server\n",
        "!rm -r processed_data\n",
        "!wget --compression=auto --header=\"Accept-Encoding: gzip\" -r -np -nH --cut-dirs=1 -R \"index.html*\" -P processed_data http://redacted.nonexistantdomain/pdata/\n",
        "\n",
        "# Directory containing the data files\n",
        "data_dir = \"processed_data/\"\n",
        "\n",
        "# File paths\n",
        "accpy_bandit_file = os.path.join(data_dir, \"accpy_bandit_all.json\")\n",
        "pypi_bandit_file = os.path.join(data_dir, \"pypi_bandit_10pct_sample.json\")\n",
        "accpy_guarddog_file = os.path.join(data_dir, \"accpy_gd_all.json\")\n",
        "pypi_guarddog_file = os.path.join(data_dir, \"pypi_gd_10pct_sample.json\")\n",
        "accpy_dynamic_file = os.path.join(data_dir, \"accpy_dynamic_all.json\")\n",
        "pypi_dynamic_file = os.path.join(data_dir, \"pypi_dynamic_10pct_sample.json\")\n",
        "survey_file = os.path.join(data_dir, \"survey.csv\")\n",
        "\n",
        "# Function to load data from a JSON file\n",
        "def load_data(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# Load data\n",
        "accpy_bandit_data = load_data(accpy_bandit_file)\n",
        "pypi_bandit_data = load_data(pypi_bandit_file)\n",
        "accpy_guarddog_data = load_data(accpy_guarddog_file)\n",
        "pypi_guarddog_data = load_data(pypi_guarddog_file)\n",
        "accpy_dynamic_data = load_data(accpy_dynamic_file)\n",
        "pypi_dynamic_data = load_data(pypi_dynamic_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oltXzSfuG-qN",
        "outputId": "7ca85e87-7089-4b10-d29b-7aed1ef99c39"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define separate labels for severity and confidence\n",
        "SEVERITY_LABELS = [\"SEVERITY.HIGH\", \"SEVERITY.MEDIUM\", \"SEVERITY.LOW\"]\n",
        "CONFIDENCE_LABELS = [\"CONFIDENCE.HIGH\", \"CONFIDENCE.MEDIUM\", \"CONFIDENCE.LOW\"]\n",
        "\n",
        "vuln_to_id = {\n",
        "    'assert_used': 1,\n",
        "    'blacklist': 2,\n",
        "    'request_without_timeout': 3,\n",
        "    'try_except_pass': 4,\n",
        "    'subprocess_without_shell_equals_true': 5,\n",
        "    'hardcoded_sql_expressions': 6,\n",
        "    'start_process_with_partial_path': 7,\n",
        "    'hardcoded_password_string': 8,\n",
        "    'start_process_with_a_shell': 9,\n",
        "    'subprocess_popen_with_shell_equals_true': 10,\n",
        "    'hashlib': 11,\n",
        "    'hardcoded_tmp_directory': 12,\n",
        "    'exec_used': 13,\n",
        "    'hardcoded_password_funcarg': 14,\n",
        "    'hardcoded_password_default': 15,\n",
        "    'hardcoded_bind_all_interfaces': 16,\n",
        "    'yaml_load': 17,\n",
        "    'django_mark_safe': 18,\n",
        "    'try_except_continue': 19,\n",
        "    'request_with_no_cert_validation': 20,\n",
        "    'jinja2_autoescape_false': 21,\n",
        "    'tarfile_unsafe_members': 22,\n",
        "    'any_other_function_with_shell_equals_true': 23,\n",
        "    'start_process_with_no_shell': 24,\n",
        "    'set_bad_file_permissions': 25,\n",
        "    'paramiko_calls': 26,\n",
        "    'ssh_no_host_key_verification': 27,\n",
        "    'django_extra_used': 28,\n",
        "    'flask_debug_true': 29,\n",
        "    'ssl_with_no_version': 30,\n",
        "    'use_of_mako_templates': 31,\n",
        "    'django_rawsql_used': 32,\n",
        "    'weak_cryptographic_key': 33,\n",
        "    'ssl_with_bad_version': 34,\n",
        "    'linux_commands_wildcard_injection': 35,\n",
        "    'snmp_insecure_version_check': 36,\n",
        "    'logging_config_insecure_listen': 37\n",
        "}\n",
        "\n",
        "# Function to aggregate issues by severity and confidence\n",
        "def aggregate_issues(data, verbose=False):\n",
        "    ks = SEVERITY_LABELS + CONFIDENCE_LABELS\n",
        "    summary = {k: 0 for k in ks}\n",
        "    s2 = {\n",
        "        \"total_issues\": 0,\n",
        "        \"total_loc\": 0,\n",
        "        \"total_packages\": 0\n",
        "    }\n",
        "    summary.update(s2)\n",
        "    heuristics = {}\n",
        "\n",
        "    for package, details in data.items():\n",
        "        for e in details[\"results\"]:\n",
        "            t = e[\"test_name\"]\n",
        "            if t not in heuristics:\n",
        "                heuristics[t] = 0\n",
        "            heuristics[t] += 1\n",
        "\n",
        "        for k in ks:\n",
        "            summary[k] += details[\"summary\"][k]\n",
        "        summary[\"total_packages\"] += 1\n",
        "        summary[\"total_issues\"] += details[\"issues\"]\n",
        "        summary[\"total_loc\"] += details[\"summary\"][\"loc\"]\n",
        "\n",
        "    print(\"----\")\n",
        "    tot = 0\n",
        "    for k,v in sorted(heuristics.items(), key=lambda e: -e[1]):\n",
        "    #    print(f\"{k}: {v}\")\n",
        "        tot += v\n",
        "    npkg = summary[\"total_packages\"]\n",
        "    print(f\"Total issues: {tot}\")\n",
        "    print(f\"Total pkg: {npkg}\")\n",
        "    print(f'Total loc: {summary[\"total_loc\"]}')\n",
        "    print(f'Total loc/pkg: {summary[\"total_loc\"] / npkg}')\n",
        "    print(\"----\")\n",
        "    return summary\n",
        "\n",
        "# Function to plot data with error bars\n",
        "def plot_comparison(accpy_data, pypi_data, metric, labels, title, ylabel):\n",
        "    # Prepare the data\n",
        "    accpy_values = [accpy_data[label] for label in labels]\n",
        "    pypi_values = [pypi_data[label] for label in labels]\n",
        "\n",
        "    # Normalize values by total issues per million lines of code\n",
        "    accpy_per_million_values = [val / accpy_data['total_loc'] * 1000000 for val in accpy_values]\n",
        "    pypi_per_million_values = [val / pypi_data['total_loc'] * 1000000 for val in pypi_values]\n",
        "\n",
        "    # Normalize values by number of issues per package\n",
        "    accpy_per_pkg_values = [val / accpy_data['total_packages'] for val in accpy_values]\n",
        "    pypi_per_pkg_values = [val / pypi_data['total_packages'] for val in pypi_values]\n",
        "\n",
        "    # Calculate error bars (Poisson distribution approximation)\n",
        "    accpy_errors = [np.sqrt(val) / accpy_data['total_loc'] * 1000000 for val in accpy_values]\n",
        "    pypi_errors = [np.sqrt(val) / pypi_data['total_loc'] * 1000000 for val in pypi_values]\n",
        "\n",
        "    accpy_pkg_errors = [np.sqrt(val) / accpy_data['total_packages'] for val in accpy_values]\n",
        "    pypi_pkg_errors = [np.sqrt(val) / pypi_data['total_packages'] for val in pypi_values]\n",
        "\n",
        "    x = range(len(labels))\n",
        "\n",
        "    # Create 2 subplots (1 row x 2 columns for the current section)\n",
        "    fig, axes = plt.subplots(1, 1, figsize=(5, 3))\n",
        "\n",
        "    # Plot for issues per million lines of code with error bars\n",
        "    accpy_errors, pypi_errors = None, None\n",
        "    bars1 = axes.bar(x, accpy_per_million_values, width=0.4, label='AccPy', align='center', yerr=accpy_errors, capsize=5)\n",
        "    bars2 = axes.bar([i + 0.4 for i in x], pypi_per_million_values, width=0.4, label='PyPI', align='center', yerr=pypi_errors, capsize=5)\n",
        "    axes.set_xticks([i + 0.2 for i in x])\n",
        "    axes.set_xticklabels(labels)#, rotation=45)\n",
        "    axes.set_ylabel(ylabel)\n",
        "    axes.set_title(f'{title} (Issues per Million Lines)')\n",
        "    axes.set_ylim([0, 11000])\n",
        "    axes.legend()\n",
        "\n",
        "    axes.bar_label(bars1, fmt='%.1f', padding=3)\n",
        "    axes.bar_label(bars2, fmt='%.1f', padding=3)\n",
        "\n",
        "    # Plot for issues per package with error bars\n",
        "    # bars3 = axes[1].bar(x, accpy_per_pkg_values, width=0.4, label='AccPy', align='center', yerr=accpy_pkg_errors, capsize=5)\n",
        "    # bars4 = axes[1].bar([i + 0.4 for i in x], pypi_per_pkg_values, width=0.4, label='PyPI', align='center', yerr=pypi_pkg_errors, capsize=5)\n",
        "    # axes[1].set_xticks([i + 0.2 for i in x])\n",
        "    # axes[1].set_xticklabels(labels, rotation=45)\n",
        "    # axes[1].set_ylabel('Number of Issues per Package')\n",
        "    # axes[1].set_title(f'{title} (Issues per Package)')\n",
        "    # axes[1].legend()\n",
        "\n",
        "    # axes[1].bar_label(bars3, fmt='%.2f', padding=3)\n",
        "    # axes[1].bar_label(bars4, fmt='%.2f', padding=3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Aggregate data\n",
        "accpy_bandit_summary = aggregate_issues(accpy_bandit_data, verbose=True)\n",
        "pypi_bandit_summary = aggregate_issues(pypi_bandit_data)\n",
        "\n",
        "# Plot comparisons for severity with error bars\n",
        "plot_comparison(accpy_bandit_summary, pypi_bandit_summary, 'Severity', SEVERITY_LABELS, 'Vulnerabilities by Severity', 'Number of Issues / mloc')\n",
        "\n",
        "# Plot comparisons for confidence with error bars\n",
        "#plot_comparison(accpy_bandit_summary, pypi_bandit_summary, 'Confidence', CONFIDENCE_LABELS, 'Vulnerabilities by Confidence', 'Number of Issues / mloc')\n",
        "\n",
        "# Additional analysis (e.g., total lines of code, total issues)\n",
        "print(f\"AccPy total number of issues: {accpy_bandit_summary['total_issues']}\")\n",
        "print(f\"PyPI total number of issues: {pypi_bandit_summary['total_issues']}\")\n",
        "print(f\"AccPy total lines of code: {accpy_bandit_summary['total_loc']}\")\n",
        "print(f\"PyPI total lines of code: {pypi_bandit_summary['total_loc']}\")\n",
        "print(f\"AccPy avg lines of code per package: {accpy_bandit_summary['total_loc'] / accpy_bandit_summary['total_packages']}\")\n",
        "print(f\"PyPI avg lines of code per package: {pypi_bandit_summary['total_loc'] / pypi_bandit_summary['total_packages']}\")\n",
        "print(f\"AccPy issues per 1 million loc: {accpy_bandit_summary['total_issues'] / accpy_bandit_summary['total_loc'] * 1000000}\")\n",
        "print(f\"PyPI issues per 1 million loc: {pypi_bandit_summary['total_issues'] / pypi_bandit_summary['total_loc'] * 1000000}\")\n",
        "print(f\"AccPy issues per package: {accpy_bandit_summary['total_issues'] / len(accpy_bandit_data)}\")\n",
        "print(f\"PyPI issues per package: {pypi_bandit_summary['total_issues'] / len(pypi_bandit_data)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "5OF3qKVuiCdn",
        "outputId": "4c0b02c9-c8b7-48f5-b72f-b1bb62ead261"
      },
      "outputs": [],
      "source": [
        "# GuardDog statistics\n",
        "\n",
        "GUARDDOG_LABELS = [\n",
        "    \"npm-silent-process-execution\", \"exec-base64\", \"silent-process-execution\",\n",
        "    \"exfiltrate-sensitive-data\", \"clipboard-access\", \"download-executable\",\n",
        "    \"obfuscation\", \"npm-exec-base64\", \"shady-links\", \"npm-install-script\",\n",
        "    \"code-execution\", \"cmd-overwrite\", \"steganography\", \"npm-serialize-environment\"\n",
        "]\n",
        "new_gd_labels = []\n",
        "gd_vuln_to_id = {\n",
        "    'empty_information': 1,\n",
        "    'single_python_file': 2,\n",
        "    'typosquatting': 3,\n",
        "    'bundled_binary': 4,\n",
        "    'release_zero': 5,\n",
        "    'deceptive_author': 6,\n",
        "    'shady-links': 7,\n",
        "    'cmd-overwrite': 8,\n",
        "    'code-execution': 9,\n",
        "    'obfuscation': 10,\n",
        "    'clipboard-access': 11,\n",
        "    'dll-hijacking': 12,\n",
        "    'exfiltrate-sensitive-data': 13,\n",
        "    'bidirectional-characters': 14,\n",
        "    'exec-base64': 15,\n",
        "    'silent-process-execution': 16,\n",
        "    'download-executable': 17\n",
        "}\n",
        "\n",
        "\n",
        "def get_gd_labels(data):\n",
        "    global new_gd_labels\n",
        "    labels = set()\n",
        "    for package, details in data.items():\n",
        "\n",
        "      labels.update(details[\"results\"].keys())\n",
        "    new_gd_labels = list(labels)\n",
        "    #new_gd_labels.remove(\"unclaimed_maintainer_email_domain\")\n",
        "    new_gd_labels.remove(\"potentially_compromised_email_domain\")\n",
        "\n",
        "def aggregate_guarddog_issues(data):\n",
        "    labels = new_gd_labels\n",
        "    summary = {\n",
        "        \"total_issues\": 0,\n",
        "        \"total_packages\": 0\n",
        "    }\n",
        "\n",
        "    summary.update({label: 0 for label in labels})\n",
        "\n",
        "    for package, details in data.items():\n",
        "        summary[\"total_issues\"] += details[\"issues\"]\n",
        "        summary[\"total_packages\"] += 1\n",
        "        for label in labels:\n",
        "            if label in details[\"results\"] and details[\"results\"][label]:\n",
        "                summary[label] += len(details[\"results\"][label]) if isinstance(details[\"results\"][label], list) else 1\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Function to plot GuardDog data\n",
        "def plot_guarddog_comparison(accpy_data, pypi_data, bandit_accpy_data, bandit_pypi_data):\n",
        "    labels = new_gd_labels\n",
        "    labels = [x for x in labels if x in gd_vuln_to_id]\n",
        "    ylabel = 'Number of Issues per Package'\n",
        "    ylabel_mloc = 'Number of Issues / mloc'\n",
        "    title = \"GuardDog Vulnerabilities Comparison (Issues per Package)\"\n",
        "    title_mloc = \"GuardDog Vulnerabilities Comparison (Issues per Million Lines of Code)\"\n",
        "\n",
        "    # Prepare the data\n",
        "    accpy_values = [accpy_data[label] for label in labels]\n",
        "    pypi_values = [pypi_data[label] for label in labels]\n",
        "\n",
        "    # Normalize values by total issues per million lines of code\n",
        "    accpy_per_pkg_values = [val / accpy_data['total_packages'] for val in accpy_values]\n",
        "    pypi_per_pkg_values = [val / pypi_data['total_packages'] for val in pypi_values]\n",
        "    accpy_per_mloc_values = [val / bandit_accpy_data['total_loc'] * 1000000 for val in accpy_values]\n",
        "    pypi_per_mloc_values = [val / bandit_pypi_data['total_loc'] * 1000000 for val in pypi_values]\n",
        "\n",
        "    # Calculate error bars\n",
        "    pypi_error = [np.sqrt(val) / pypi_data['total_packages'] for val in pypi_values]\n",
        "    accpy_error = [np.sqrt(val) / accpy_data['total_packages'] for val in accpy_values]\n",
        "    accpy_per_mloc_error = [np.sqrt(val) / bandit_accpy_data['total_loc'] * 1000000 for val in accpy_values]\n",
        "    pypi_per_mloc_error = [np.sqrt(val) / bandit_pypi_data['total_loc'] * 1000000 for val in pypi_values]\n",
        "\n",
        "    x = range(len(labels))\n",
        "    labels = [ gd_vuln_to_id[x] if x in gd_vuln_to_id else x for x in labels ]\n",
        "\n",
        "    # Create subplots\n",
        "    # plt.figure(figsize=(12, 6))\n",
        "    fig, axes = plt.subplots(1, 1, figsize=(6, 3))\n",
        "    # axes[0].bar(x, accpy_per_pkg_values, width=0.4, label='AccPy', align='center', yerr=accpy_error, capsize=5)\n",
        "    # axes[0].bar([i + 0.4 for i in x], pypi_per_pkg_values, width=0.4, label='PyPI', align='center', yerr=pypi_error, capsize=5)\n",
        "    # x = list(x)\n",
        "    # axes[0].set_xticks([i + 0.2 for i in x], labels, rotation=30, ha='right')\n",
        "    # axes[0].set_ylabel(ylabel)\n",
        "    # axes[0].set_title(title)\n",
        "    # axes[0].legend()\n",
        "\n",
        "    axes.bar(x, accpy_per_mloc_values, width=0.4, label='AccPy', align='center', capsize=5)\n",
        "    axes.bar([i + 0.4 for i in x], pypi_per_mloc_values, width=0.4, label='PyPI', align='center', capsize=5)\n",
        "    axes.set_xticks([i + 0.2 for i in x], labels, rotation=30, ha='right')\n",
        "    axes.set_ylabel(ylabel_mloc)\n",
        "    axes.set_title(title_mloc)\n",
        "    axes.legend()\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "# Aggregate data for both\n",
        "get_gd_labels({**accpy_guarddog_data, **pypi_guarddog_data})\n",
        "accpy_summary = aggregate_guarddog_issues(accpy_guarddog_data)\n",
        "pypi_summary = aggregate_guarddog_issues(pypi_guarddog_data)\n",
        "\n",
        "# Plot comparison\n",
        "plot_guarddog_comparison(accpy_summary, pypi_summary, accpy_bandit_summary, pypi_bandit_summary)\n",
        "\n",
        "# Additional analysis\n",
        "print(f\"AccPy total number of issues: {accpy_summary['total_issues']}\")\n",
        "print(f\"PyPI total number of issues: {pypi_summary['total_issues']}\")\n",
        "print(f\"AccPy total number of packages: {accpy_summary['total_packages']}\")\n",
        "print(f\"PyPI total number of packages: {pypi_summary['total_packages']}\")\n",
        "print(f\"AccPy issues per package: {accpy_summary['total_issues'] / accpy_summary['total_packages']}\")\n",
        "print(f\"PyPI issues per package: {pypi_summary['total_issues'] / pypi_summary['total_packages']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Q0UnO4_Bwa",
        "outputId": "121c7f7e-e7be-4239-b55c-96a5ba70ceea"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "def gd_data_to_vulnlist(gd_data):\n",
        "    res = {}\n",
        "    vuln_counter = Counter()\n",
        "\n",
        "    for pkg in gd_data:\n",
        "        res[pkg] = []\n",
        "        for vuln in gd_data[pkg][\"results\"]:\n",
        "            if gd_data[pkg][\"results\"][vuln] in [None, {}, \"\"]:\n",
        "                continue\n",
        "            res[pkg].append(vuln)\n",
        "            vuln_counter[vuln] += 1\n",
        "\n",
        "    # Get top 10 vulnerabilities by count\n",
        "    top_vulns = vuln_counter.most_common(30)\n",
        "\n",
        "    # Count unique occurrences of each vulnerability\n",
        "    unique_vuln_counts = {vuln: sum(1 for pkgs in res.values() if vuln in pkgs) for vuln, _ in top_vulns}\n",
        "\n",
        "    # Create a DataFrame for better visualization\n",
        "    df = pd.DataFrame(top_vulns, columns=[\"Vulnerability\", \"Count\"])\n",
        "    df[\"Unique Occurrences\"] = df[\"Vulnerability\"].map(unique_vuln_counts)\n",
        "\n",
        "    return df\n",
        "\n",
        "#top_vuln_df = gd_data_to_vulnlist(accpy_guarddog_data)\n",
        "#display(top_vuln_df)\n",
        "\n",
        "#top_vuln_df = gd_data_to_vulnlist(pypi_guarddog_data)\n",
        "#display(top_vuln_df)\n",
        "\n",
        "import pprint\n",
        "\n",
        "def bandit_data_to_vulnlist(bandit_data):\n",
        "  l = { k: e for k,e in bandit_data.items() if e[\"issues\"] > 0 }\n",
        "  vlist = {}\n",
        "  for p in l:\n",
        "    vlist[p] = [x[\"test_name\"] for x in bandit_data[p][\"results\"]]\n",
        "  cnts = {}\n",
        "  for k,v in vlist.items():\n",
        "    for e in v:\n",
        "      if e not in cnts:\n",
        "        cnts[e] = 0\n",
        "      cnts[e] += 1\n",
        "  pprint.pp(cnts)\n",
        "\n",
        "bandit_data_to_vulnlist(accpy_bandit_data)\n",
        "bandit_data_to_vulnlist(pypi_bandit_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DODcdmwXtj5n",
        "outputId": "2e730eda-7477-4bf0-c593-2ea2358ab7e6"
      },
      "outputs": [],
      "source": [
        "# Dynamic statistics\n",
        "\n",
        "def extract_dynamic_data(data):\n",
        "    packages = []\n",
        "\n",
        "    for package_name, details in data.items():\n",
        "        package_data = {\n",
        "            \"name\": package_name,\n",
        "            \"packets\": details[\"packets\"],\n",
        "            \"dep_packets\": details[\"dep_packets\"],\n",
        "            \"packets_size\": details[\"packets_size\"],\n",
        "            \"dep_packets_size\": details[\"dep_packets_size\"],\n",
        "            \"packets_domains\": details[\"packets_domains\"],\n",
        "            \"dep_packets_domains\": details[\"dep_packets_domains\"]\n",
        "        }\n",
        "        packages.append(package_data)\n",
        "\n",
        "    return packages\n",
        "\n",
        "# Extract dynamic data for both AccPy and PyPI\n",
        "accpy_packages = extract_dynamic_data(accpy_dynamic_data)\n",
        "pypi_packages = extract_dynamic_data(pypi_dynamic_data)\n",
        "\n",
        "# Create a scatter plot with different colors for AccPy and PyPI, one dot per package\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Collect data for AccPy\n",
        "accpy_packets = [pkg[\"packets\"] for pkg in accpy_packages]\n",
        "accpy_packets_size = [pkg[\"packets_size\"] / pkg[\"packets\"] if pkg[\"packets\"] > 0 else 0 for pkg in accpy_packages]\n",
        "accpy_dep_packets = [pkg[\"dep_packets\"] for pkg in accpy_packages]\n",
        "accpy_dep_packets_size = [pkg[\"dep_packets_size\"] / pkg[\"dep_packets\"] if pkg[\"dep_packets\"] > 0 else 0 for pkg in accpy_packages]\n",
        "\n",
        "# Collect data for PyPI\n",
        "pypi_packets = [pkg[\"packets\"] for pkg in pypi_packages]\n",
        "pypi_packets_size = [pkg[\"packets_size\"] / pkg[\"packets\"] if pkg[\"packets\"] > 0 else 0 for pkg in pypi_packages]\n",
        "pypi_dep_packets = [pkg[\"dep_packets\"] for pkg in pypi_packages]\n",
        "pypi_dep_packets_size = [pkg[\"dep_packets_size\"] / pkg[\"dep_packets\"] if pkg[\"dep_packets\"] > 0 else 0 for pkg in pypi_packages]\n",
        "\n",
        "# Plot all AccPy data points in one go\n",
        "ax.scatter(accpy_packets, accpy_packets_size, color='blue', label='AccPy - Packets', s=100, alpha=0.6)\n",
        "ax.scatter(accpy_dep_packets, accpy_dep_packets_size, color='lightblue', label='AccPy - Dep Packets', s=100, alpha=0.6)\n",
        "\n",
        "# Plot all PyPI data points in one go\n",
        "ax.scatter(pypi_packets, pypi_packets_size, color='red', label='PyPI - Packets', s=100, alpha=0.6)\n",
        "ax.scatter(pypi_dep_packets, pypi_dep_packets_size, color='orange', label='PyPI - Dep Packets', s=100, alpha=0.6)\n",
        "\n",
        "\n",
        "# Remove duplicate labels in the legend\n",
        "handles, labels = ax.get_legend_handles_labels()\n",
        "unique_labels = dict(zip(labels, handles))\n",
        "ax.legend(unique_labels.values(), unique_labels.keys())\n",
        "\n",
        "# Labeling the plot\n",
        "ax.set_xlabel('Number of TCP+UDP Packets')\n",
        "ax.set_ylabel('Avg Packet Size (bytes)')\n",
        "ax.set_title('Dynamic Analysis: Packets vs. Packet Size Comparison (AccPy vs PyPI)')\n",
        "ax.set_yscale('log')\n",
        "ax.set_xscale('log')\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "fig.set_dpi(100)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Additional statistics\n",
        "print(f\"AccPy total number of packets: {sum(accpy_packets)}\")\n",
        "print(f\"AccPy total number of dependent packets: {sum(accpy_dep_packets)}\")\n",
        "print(f\"AccPy total packets size: {sum(accpy_packets_size)} bytes\")\n",
        "print(f\"AccPy total dependent packets size: {sum(accpy_dep_packets_size)} bytes\")\n",
        "print(f\"AccPy total unique domains: {len(set([item for sublist in [pkg['packets_domains'] for pkg in accpy_packages] for item in sublist]))}\")\n",
        "\n",
        "print(f\"PyPI total number of packets: {sum(pypi_packets)}\")\n",
        "print(f\"PyPI total number of dependent packets: {sum(pypi_dep_packets)}\")\n",
        "print(f\"PyPI total packets size: {sum(pypi_packets_size)} bytes\")\n",
        "print(f\"PyPI total dependent packets size: {sum(pypi_dep_packets_size)} bytes\")\n",
        "print(f\"PyPI total unique domains: {len(set([item for sublist in [pkg['packets_domains'] for pkg in pypi_packages] for item in sublist]))}\")\n",
        "print(f\"PyPI number of packages with at least one packet: {len([ x for x in pypi_packets if x > 0 ])}\")\n",
        "\n",
        "# Print all domains\n",
        "with_telemetry = 0\n",
        "without_telemetry = 0\n",
        "for s in accpy_packages:\n",
        "    p = s['name']\n",
        "    WHITELISTED_DOMAINS = [\"pypi.org\", \"files.pythonhosted.org\"]\n",
        "    d1 = [x for x in s['packets_domains'] if x not in WHITELISTED_DOMAINS]\n",
        "    d2 = [x for x in s['dep_packets_domains'] if x not in WHITELISTED_DOMAINS]\n",
        "\n",
        "    if len(d1) > 0:\n",
        "      print(f\"{p} - {d1}\");\n",
        "      with_telemetry += 1\n",
        "    else:\n",
        "      without_telemetry += 1\n",
        "    #if len(d2) > 0:\n",
        "    #print(f\"{p} - {d2}\");\n",
        "\n",
        "print(f\"AccPy packages with telemetry: {with_telemetry}\")\n",
        "print(f\"AccPy packages without telemetry: {without_telemetry}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8MSSt4d35_j",
        "outputId": "a4ed7ce7-9749-406b-910c-9fe70b852eb5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import comb\n",
        "import scipy.stats as st\n",
        "\n",
        "accpy_data = []\n",
        "for pkg in accpy_packages:\n",
        "    if pkg[\"packets\"] > 0:\n",
        "        accpy_data.append([pkg[\"packets\"], pkg[\"packets_size\"] / pkg[\"packets\"]])\n",
        "    if pkg[\"dep_packets\"] > 0:\n",
        "        accpy_data.append([pkg[\"dep_packets\"], pkg[\"dep_packets_size\"] / pkg[\"dep_packets\"]])\n",
        "\n",
        "accpy_data = np.array(accpy_data)\n",
        "\n",
        "pypi_data = []\n",
        "for pkg in pypi_packages:\n",
        "    if pkg[\"packets\"] > 0:\n",
        "        pypi_data.append([pkg[\"packets\"], pkg[\"packets_size\"] / pkg[\"packets\"]])\n",
        "    if pkg[\"dep_packets\"] > 0:\n",
        "        pypi_data.append([pkg[\"dep_packets\"], pkg[\"dep_packets_size\"] / pkg[\"dep_packets\"]])\n",
        "\n",
        "pypi_data = np.array(pypi_data)\n",
        "\n",
        "print(\"AccPy data shape:\", accpy_data.shape)\n",
        "print(\"PyPI data shape:\", pypi_data.shape)\n",
        "\n",
        "def hotelling_t2_test(X, Y):\n",
        "    \"\"\"\n",
        "    Perform two-sample Hotelling T^2 test on two 2D samples:\n",
        "      X: n1 x d\n",
        "      Y: n2 x d\n",
        "    Returns T^2 statistic, F statistic, and p-value.\n",
        "    \"\"\"\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "\n",
        "    n1, d = X.shape\n",
        "    n2, _ = Y.shape\n",
        "\n",
        "    # Means\n",
        "    mean_X = np.mean(X, axis=0)\n",
        "    mean_Y = np.mean(Y, axis=0)\n",
        "    diff_mean = mean_X - mean_Y\n",
        "\n",
        "    # Covariances\n",
        "    Sx = np.cov(X, rowvar=False, bias=False)\n",
        "    Sy = np.cov(Y, rowvar=False, bias=False)\n",
        "\n",
        "    Sp = ((n1 - 1) * Sx + (n2 - 1) * Sy) / (n1 + n2 - 2)\n",
        "\n",
        "    Sp_inv = np.linalg.inv(Sp)\n",
        "\n",
        "    T2 = (n1 * n2) / (n1 + n2) * diff_mean @ Sp_inv @ diff_mean\n",
        "\n",
        "    numerator_df = d\n",
        "    denominator_df = n1 + n2 - d - 1\n",
        "\n",
        "    if denominator_df <= 0:\n",
        "        raise ValueError(\"Not enough observations to perform Hotelling's T^2 test.\")\n",
        "\n",
        "    F = ((n1 + n2 - d - 1) / (d * (n1 + n2 - 2))) * T2\n",
        "\n",
        "    p_value = 1 - st.f.cdf(F, numerator_df, denominator_df)\n",
        "\n",
        "    return T2, F, p_value\n",
        "\n",
        "T2, F_stat, p_val = hotelling_t2_test(accpy_data, pypi_data)\n",
        "print(f\"Hotelling T^2 = {T2:.4f}\")\n",
        "print(f\"F statistic   = {F_stat:.4f}\")\n",
        "print(f\"p-value       = {p_val:.6g}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPq4TmxbKfOT",
        "outputId": "10f41362-4be5-40ee-a1db-bbdd331e1fde"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def random_subsample(X, size=1000, random_state=None):\n",
        "    \"\"\"\n",
        "    Return 'size' random rows from X (without replacement).\n",
        "    If X is smaller than 'size', return X as-is.\n",
        "    \"\"\"\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    n = len(X)\n",
        "    if n <= size:\n",
        "        return X\n",
        "    idx = np.random.choice(n, size=size, replace=False)\n",
        "    return X[idx]\n",
        "\n",
        "def energy_distance_nd(X, Y):\n",
        "    \"\"\"\n",
        "    Compute the generalized energy distance between two samples X and Y\n",
        "    in any dimension d, using Euclidean distance.\n",
        "    \"\"\"\n",
        "    d_xx = cdist(X, X)\n",
        "    d_yy = cdist(Y, Y)\n",
        "    d_xy = cdist(X, Y)\n",
        "    mean_xx = d_xx.mean()\n",
        "    mean_yy = d_yy.mean()\n",
        "    mean_xy = d_xy.mean()\n",
        "    return 2.0 * mean_xy - mean_xx - mean_yy\n",
        "\n",
        "def two_sample_energy_test_nd(X, Y, n_permutations=1000, subsample_size=1000, random_state=None):\n",
        "    \"\"\"\n",
        "    Permutation-based E-test on d-dimensional data, using random subsampling\n",
        "    to avoid out-of-memory errors with large datasets.\n",
        "    \"\"\"\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    X_sub = random_subsample(X, size=subsample_size, random_state=random_state)\n",
        "    Y_sub = random_subsample(Y, size=subsample_size, random_state=random_state)\n",
        "\n",
        "    stat_obs = energy_distance_nd(X_sub, Y_sub)\n",
        "\n",
        "    n1 = len(X_sub)\n",
        "    n2 = len(Y_sub)\n",
        "    pooled = np.vstack([X_sub, Y_sub])\n",
        "    count = 0\n",
        "    for _ in range(n_permutations):\n",
        "        np.random.shuffle(pooled)  # shuffle in place\n",
        "        Xp = pooled[:n1]\n",
        "        Yp = pooled[n1:]\n",
        "        stat_perm = energy_distance_nd(Xp, Yp)\n",
        "        if stat_perm >= stat_obs:\n",
        "            count += 1\n",
        "\n",
        "    p_value = (count + 1.0) / (n_permutations + 1.0)\n",
        "    return stat_obs, p_value\n",
        "\n",
        "accpy_x = accpy_packets + accpy_dep_packets\n",
        "accpy_y = accpy_packets_size + accpy_dep_packets_size\n",
        "accpy_data = np.column_stack([accpy_x, accpy_y])\n",
        "\n",
        "pypi_x = pypi_packets + pypi_dep_packets\n",
        "pypi_y = pypi_packets_size + pypi_dep_packets_size\n",
        "pypi_data = np.column_stack([pypi_x, pypi_y])\n",
        "\n",
        "\n",
        "stat, p_val = two_sample_energy_test_nd(\n",
        "    X=accpy_data,\n",
        "    Y=pypi_data,\n",
        "    n_permutations=1000,\n",
        "    subsample_size=2000,   # choose a feasible sample size\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Approx. Energy Distance on subsample:\", stat)\n",
        "print(\"Permutation test p-value:\", p_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL3ZDR0kXRzB"
      },
      "outputs": [],
      "source": [
        "# Load package lists from files\n",
        "def load_package_list(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return set(line.strip() for line in f)\n",
        "\n",
        "accpy_packages = load_package_list('processed_data/accpy_index_new.list')\n",
        "pypi_packages = load_package_list('processed_data/index.list')\n",
        "accpy_packages.update({\"availsim4\", \"certifi\", \"cmmnbuild-dep-manager\", \"comrad\", \"lhcsmapi\", \"longitudinal-tomography\", \"midas\", \"oaf\", \"pjlsa\", \"pybt\", \"pyda\", \"pydaq\", \"pyjapc\", \"pyrbac\", \"python-env\", \"pytimber\", \"stubgenj\"})\n",
        "\n",
        "# Find AccPy packages not in PyPI using set difference\n",
        "accpy_not_in_pypi = accpy_packages - pypi_packages\n",
        "\n",
        "# Calculate percentages\n",
        "total_accpy_packages = len(accpy_packages)\n",
        "total_pypi_packages = len(pypi_packages)\n",
        "accpy_not_in_pypi_count = len(accpy_not_in_pypi)\n",
        "accpy_in_pypi_count = total_accpy_packages - accpy_not_in_pypi_count\n",
        "\n",
        "# Create pie chart\n",
        "labels = ['Acc-Py packages not in PyPI', 'Acc-Py packages in PyPI']\n",
        "sizes = [accpy_not_in_pypi_count, accpy_in_pypi_count]\n",
        "colors = ['lightcoral', 'lightskyblue']\n",
        "explode = (0.1, 0)  # Explode the first slice (AccPy not in PyPI)\n",
        "\n",
        "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
        "        autopct='%1.1f%%', shadow=True, startangle=140, textprops={'fontsize': 16})\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.title('Acc-Py Package Presence in PyPI', fontsize=20)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Total AccPy packages: {total_accpy_packages}\")\n",
        "print(f\"AccPy packages not in PyPI: {accpy_not_in_pypi_count}\")\n",
        "print(f\"Total PyPi packages: {total_pypi_packages}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Copc1_Rm-tuW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import textwrap\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(survey_file)\n",
        "\n",
        "# Define the Yes/No and Likert scale columns\n",
        "yes_no_columns = [\n",
        "    'Prior to this demonstration, did you have any knowledge of how a dependency confusion attack works? (cf. https://owasp.org/www-project-top-10-ci-cd-security-risks/CICD-SEC-03-Dependency-Chain-Abuse)\\n',\n",
        "    'Prior to this demonstration, were you aware that the acc-py environment could be affected by dependency confusion vulnerabilities?\\n',\n",
        "    'Do you think that the solution provided is useful?',\n",
        "    'Were you aware of the potential privacy problems of telemetry?',\n",
        "    'Were you aware of all the informations collected by acc-py?'\n",
        "]\n",
        "\n",
        "titles = [\n",
        "    '(Q4) Prior to this demonstration, did you have any knowledge of how a dependency confusion attack works?',\n",
        "    '(Q5) Prior to this demonstration, were you aware that the Acc-Py environment could be targeted by dependency confusion attacks?',\n",
        "    '(Q8) Do you think that the solution to counter dependency confusion attacks we proposed is useful?',\n",
        "    '(Q10) Were you aware of the potential privacy problems of telemetry?',\n",
        "    '(Q11) Were you aware of all the informations collected by Acc-Py?'\n",
        "]\n",
        "\n",
        "likert_columns = [\n",
        "    'How relevant do you believe this issue is for acc-py?',\n",
        "    'How severe do you find dependency confusion attacks?',\n",
        "    'Do you think this might be a problem?'\n",
        "]\n",
        "title_overrides = [\n",
        "    '(Q6) How relevant do you believe dependency confusion attacks are for Acc-Py?',\n",
        "    '(Q7) How severe do you find dependency confusion attacks?',\n",
        "    '(Q13) Do you think the current usage of telemetry in Acc-Py might be a problem?',\n",
        "]\n",
        "\n",
        "def wrap_title(text, width=72):\n",
        "    \"\"\"Wrap a title string to the given width.\"\"\"\n",
        "    return '\\n'.join(textwrap.wrap(text, width))\n",
        "\n",
        "# Possible responses we want to ensure always appear\n",
        "yes_no_responses = [\"Yes\", \"No\"]\n",
        "\n",
        "# --- Horizontal Bar Charts for Yes/No Columns ---\n",
        "fig, axes = plt.subplots(len(yes_no_columns), 1, figsize=(6, 4), sharex=True)\n",
        "\n",
        "for i, column in enumerate(yes_no_columns):\n",
        "    ax = axes[i]\n",
        "    # Count the occurrences of each response\n",
        "    counts = df[column].value_counts()\n",
        "\n",
        "    # Reindex so both 'Yes' and 'No' appear, even if one is missing\n",
        "    counts = counts.reindex(yes_no_responses, fill_value=0)\n",
        "\n",
        "    # Create a horizontal bar chart\n",
        "    #ax.barh(counts.index.astype(str), counts.values)\n",
        "    WD = 0.01\n",
        "    ax.barh([\"No\"], [-counts.values[1]], color=\"#a83232\", height=WD)\n",
        "    ax.twinx().barh([\"Yes\"], [counts.values[0]], color=\"#42a832\", height=WD)\n",
        "\n",
        "    # Wrap the title to ensure it fits nicely in the width\n",
        "    ax.set_title(wrap_title(titles[i]), fontsize=10, loc='left')\n",
        "\n",
        "    ax.set_xticks([-4,-3,-2,-1,0,1,2,3,4])\n",
        "    ax.set_xticklabels([4,3,2,1,0,1,2,3,4])\n",
        "\n",
        "    # Only label the bottom plot with an X-label to save space\n",
        "    if i < len(yes_no_columns) - 1:\n",
        "        ax.set_xlabel('')\n",
        "    else:\n",
        "        ax.set_xlabel('Answer count')\n",
        "\n",
        "# Adjust subplot spacing, then use tight_layout\n",
        "plt.subplots_adjust(hspace=0.05)  # Adjust vertical spacing as needed\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Likert Scale Columns (Line + Error Bar) ---\n",
        "fig, axes = plt.subplots(len(likert_columns), 1, figsize=(6, 4))\n",
        "\n",
        "# Loop through the Likert columns to calculate and plot averages\n",
        "for i, column in enumerate(likert_columns):\n",
        "    ax = axes[i]\n",
        "\n",
        "    # Calculate the average and standard deviation\n",
        "    average_score = df[column].mean()\n",
        "    std_dev = df[column].std()\n",
        "\n",
        "    # Plot a horizontal line representing the scale (from 0 to 10)\n",
        "    ax.plot([0, 10], [1, 1], color='gray', lw=2)  # The scale line\n",
        "\n",
        "    # Mark the average score with a red dot and error bar\n",
        "    ax.errorbar(\n",
        "        average_score,\n",
        "        1,\n",
        "        xerr=std_dev,\n",
        "        fmt='o',\n",
        "        color='red',\n",
        "        markersize=10,\n",
        "        capsize=5\n",
        "    )\n",
        "\n",
        "    # Annotate the average score\n",
        "    ax.text(\n",
        "        average_score,\n",
        "        1.02,\n",
        "        f'Avg: {average_score:.2f}',\n",
        "        horizontalalignment='center',\n",
        "        fontsize=10\n",
        "    )\n",
        "    ax.text(\n",
        "        average_score,\n",
        "        0.96,\n",
        "        f'SD: {std_dev:.2f}',\n",
        "        horizontalalignment='center',\n",
        "        fontsize=10\n",
        "    )\n",
        "\n",
        "    # Set the x-axis limits and labels\n",
        "    ax.set_xlim(0, 10)\n",
        "    ax.set_xticks(range(0, 11))\n",
        "\n",
        "    # Remove y-axis ticks and labels (since this is a single line)\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    # Set the title of the subplot\n",
        "    ax.set_title(wrap_title(title_overrides[i]), fontsize=10, loc='left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
